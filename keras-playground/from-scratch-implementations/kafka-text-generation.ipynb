{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka = open('kafka.txt').read()\n",
    "uniqueChars = list(set(kafka))\n",
    "\n",
    "charToIndex = { char:index for index, char in enumerate(uniqueChars) } \n",
    "indexToChar = { index:char for index, char in enumerate(uniqueChars) } \n",
    "\n",
    "encodingDimens = len(uniqueChars) + 1\n",
    "def encode(character):\n",
    "    vector = np.zeros((encodingDimens, 1))\n",
    "    vector[charToIndex[character]] = 1\n",
    "    return vector\n",
    "def decode(vector):\n",
    "    charIndex = [index for index in range(len(vector)) if vector[index] == 1][0]\n",
    "    return indexToChar[charIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x * (x > 0)\n",
    "def reluPrime(x):\n",
    "    return 1. * (x > 0)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def softmaxPrime(x, y):\n",
    "    result = np.copy(x)\n",
    "    result[y] -= 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddenUnits = 100\n",
    "batchLength = 25\n",
    "learningRate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hiddenUnits, encodingDimens) * 0.01\n",
    "Whh = np.random.randn(hiddenUnits, hiddenUnits) * 0.01 \n",
    "Why = np.random.randn(encodingDimens, hiddenUnits) * 0.01 \n",
    "bh = np.zeros((hiddenUnits, 1))\n",
    "by = np.zeros((encodingDimens, 1))\n",
    "\n",
    "def predict(X):\n",
    "    batchLength = len(X)\n",
    "    \n",
    "    a0, a1, a2 = {}, {}, {}\n",
    "    z1, z2 = {}, {}\n",
    "    \n",
    "    y = {}\n",
    "    \n",
    "    previousHiddenState = {}\n",
    "    previousHiddenState[0] = np.zeros((hiddenUnits, 1))\n",
    "\n",
    "    for time in range(batchLength):\n",
    "        a0[time] = X[time]\n",
    "        z1[time] = np.dot(Wxh, a0[time]) + np.dot(Whh, previousHiddenState[time]) + bh\n",
    "        a1[time] = relu(z1[time])\n",
    "    \n",
    "        z2[time] = np.dot(Why, a1[time]) + by\n",
    "        a2[time] = softmax(z2[time])\n",
    "        \n",
    "        previousHiddenState[time+1] = a1[time];\n",
    "        \n",
    "    return a2[batchLength-1];\n",
    "\n",
    "def loss(X, y):\n",
    "    batchLength = len(X)\n",
    "    crossEntropy = 0\n",
    "    loss, gradients = 0, {}\n",
    "    \n",
    "    a0, a1, a2 = {}, {}, {}\n",
    "    z1, z2 = {}, {}\n",
    "    \n",
    "    previousHiddenState = {}\n",
    "    previousHiddenState[0] = np.zeros((hiddenUnits, 1))\n",
    "\n",
    "    for time in range(batchLength):\n",
    "        a0[time] = X[time]\n",
    "        z1[time] = np.dot(Wxh, a0[time]) + np.dot(Whh, previousHiddenState[time]) + bh\n",
    "        a1[time] = relu(z1[time])\n",
    "    \n",
    "        z2[time] = np.dot(Why, a1[time]) + by\n",
    "        a2[time] = softmax(z2[time])\n",
    "        \n",
    "        crossEntropy += -np.log(a2[time][y[time],0])      \n",
    "        previousHiddenState[time+1] = a1[time];\n",
    "    \n",
    "    gradWxh, gradWhh, gradWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    gradbh, gradby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    deltaStatePrevious = np.zeros_like(previousHiddenState[0])\n",
    "    \n",
    "    delta3, delta2 = {}, {}\n",
    "    for time in reversed(range(batchLength)):\n",
    "        delta3             = softmaxPrime(X[time], y[time])     \n",
    "        delta2             = np.dot(Why.T, delta3) + deltaStatePrevious\n",
    "        deltaStatePrevious = np.dot(Whh.T, delta2)\n",
    "        \n",
    "        gradby += delta3\n",
    "        gradbh += delta2\n",
    "        \n",
    "        gradWhy += np.dot(delta3, a1[time].T)\n",
    "        gradWxh += np.dot(delta2 * reluPrime(z1[time]), a0[time].T)\n",
    "        gradWhh += np.dot(delta2 * reluPrime(z1[time]), previousHiddenState[time].T)\n",
    "\n",
    "    return crossEntropy, (gradWhy, gradWhh, gradWxh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81, 100) (100, 100) (100, 81)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([encode('O'), encode('n')])\n",
    "y = np.array([charToIndex['n'], charToIndex['e']])\n",
    "\n",
    "entropy, grads = loss(X, y)\n",
    "\n",
    "print(grads[0].shape, grads[1].shape, grads[2].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
